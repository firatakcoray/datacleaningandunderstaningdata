##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################
####################---------------------------------------###############################
########Python beneficial scripts while working cleaning and understanding data###########
###################----------------------------------------###############################
##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################

# To import pandas and load the dataframe
import pandas as pd
df = pd.read_csv('literary_birth_rate.csv')



# To check first and last rows of your dataframe this will help you to understand the data roughly.
df.head()
df.tail()
  


# check the columns of your dataframe - be careful of column names with spaces and rename them if there is a column like that
# This will be important while you are fetching data by using pandas data manipulation tools.
df.columns
df.rename(columns = {'t e s t':'TEST'}, inplace = True)



# check the shape of the data frame # of cols and rows -  for example if its a dataset that has countries and if 
# the row number is less than all countries on world it can give an idea about bugs of the dataset.
df.shape



# get the info of your data frame - column names, types of data on each column, how many null rows in each column etc.
# you have to handle this missing data - another course about this.
# some columns may have numeric values and can actually be stored as object (string) so those should be handled.
df.info()



# round to two decimal places in python pandas for column values that are big numbers
# Format with commas and round off to two decimal places in pandas 
pd.options.display.float_format = '{:.2f}'.format
pd.set_option('display.float_format', lambda x: '%.3f' % x)



# Format with dollars, commas and round off to two decimal places in pandas 
pd.options.display.float_format = '${:, .2f}'.format



# formatting date data in your data frame
df["Date"] = df["Date"].dt.strftime("%m-%d-%Y")



# columns should be analyzed regarding unique values and frequency analysis
# continent column has 49 rows as AF value seems Africa continent appears 
# 49 times at the dataset - drop = False helps to count missing values
# Sweden country appears twice which is probably an error
df.continent.value_counts(dropna=False)
df['continent'].value_counts(dropna=False)
df.country.value_counts(dropna=False).head()



# calculate summary statistics on our numeric (only numeric columns!!!) data also very helpful. 
# Outliers can be detected and it need further investigation to handle.
# Not all outliers are bad data points some can be error but some can be valid.
df.describe()


# !!! histogramlari cidirirken x ve y axislerin isimlendirilmesi veri kutularinin aralklarinin set edilmesi vs. kodlari eklenmeli




# plotting histograms help to see data distribution of continuous data counts - bar plots are used for discrete data!!!
# helps us to look at frequencies of our data. Histogram how many sorularina cevap vermemiz konusunda bize kolaylik saglar.
# verininin rakamsal olarak nasi dagildigi hangi deger araliklarinda kac adet veri noktasi bulundugunu gormemizi saglar.
import matplotlib.pyplot as plt 
df.population.plot('hist')
plt.show()



# Plot the histogram - logx logy true helps to scale the data values to meaningfully show on histogram
df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)
# If you find a problem on your data (like outliars) u can slice the data and check the problem.
df[df.pooulation > 1000000000]


# box plots to visualize summary statistics - box plot veri noktalarinin degersel olarak nasil dagildigi konsunda bize destek olur
# medyani bir cirpida bulmamizi, veri kumesinin ne derece daginink oldugunu gormemizi ve outliarlari bulmamizi saglar.
df.boxplot(column='population', by='continent')
plt.show()



# scatter plots are used to show relationship between 2 numeric columns - used for identifying bad data.
# 


### Tidy data for analysis ###
# There are data formats better for analyzing and data formats better for reporting. We have to pick the way we go and format the data accordingly.
# To fix problem that columns contains values instead of variables we use pd.melt() id_vars is the paremeter that we want to keep fixed. 
pd.melt(frame=df, id_vars='name', value_vars=['treatment a', 'treatment b'],var_name='treatment', value_name='result')

# Opposite of melting is pivoting. We convert columns to rows in melting and now in pivoting we take each unique value of a variable and show that into columns. 
# Pivoting is Report friendly shape. Melting is analysis friendly shape. 
weather_tidy = weather.pivot(index='date', columns='element',values='value')


# If we havae duplicate rows we can get easily errors. We need to specify aggregate function eg.mean and use pivot table - index is the column you want to fix
# value is the column you want to aggregate and columns are the columns you want to pivot on.
weather2_tidy = weather.pivot_table(values='value', index='date', columns='element', aggfunc=np.mean)


# After pivotin a dataframe you get a totally new DF that has a new index as hieararchical index. How you can get back to original DF as. 
df.reset_index()

#tb_melt['sex'] = tb_melt.variable.str[0] -> str ile columndaki deger uzerinde oynama yapilabilir. Liste de olsa string de olsa str ile erisiyorsun
# Create the 'str_split' column
ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')

# Create the 'type' column
ebola_melt['type'] = ebola_melt.str_split.str[0]

# Create the 'country' column
ebola_melt['country'] = ebola_melt.str_split.str[1]

# Print the head of ebola_melt
print(ebola_melt.head())
             Date  Day  type_country  counts        str_split   type country
    0    1/5/2015  289  Cases_Guinea  2776.0  [Cases, Guinea]  Cases  Guinea
    1    1/4/2015  288  Cases_Guinea  2775.0  [Cases, Guinea]  Cases  Guinea
    2    1/3/2015  287  Cases_Guinea  2769.0  [Cases, Guinea]  Cases  Guinea
    3    1/2/2015  286  Cases_Guinea     NaN  [Cases, Guinea]  Cases  Guinea
    4  12/31/2014  284  Cases_Guinea  2730.0  [Cases, Guinea]  Cases  Guinea






##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################
####-------------------Datacamp - Data Manipulation with Pandas-----------------------####3
##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################

# Print the head of the homelessness data
print(homelessness.head())
               region       state  individuals  family_members  state_pop
0  East South Central     Alabama       2570.0           864.0    4887681
1             Pacific      Alaska       1434.0           582.0     735139
2            Mountain     Arizona       7259.0          2606.0    7158024
3  West South Central    Arkansas       2280.0           432.0    3009733
4             Pacific  California     109008.0         20964.0   39461588




# Print information about homelessness
print(homelessness.info())
<class 'pandas.core.frame.DataFrame'>
Int64Index: 51 entries, 0 to 50
Data columns (total 5 columns):
region            51 non-null object
state             51 non-null object
individuals       51 non-null float64
family_members    51 non-null float64
state_pop         51 non-null int64
dtypes: float64(2), int64(1), object(2)
memory usage: 2.4+ KB
None




# Print the shape of homelessness
print(homelessness.shape)
(51, 5)



# Print a description of homelessness integer columns
print(homelessness.describe())
       individuals  family_members  state_pop
count       51.000          51.000  5.100e+01
mean      7225.784        3504.882  6.406e+06
std      15991.025        7805.412  7.327e+06
min        434.000          75.000  5.776e+05
25%       1446.500         592.000  1.777e+06
50%       3082.000        1482.000  4.461e+06
75%       6781.500        3196.000  7.341e+06
max     109008.000       52070.000  3.946e+07




# Print the values of homelessness
print(homelessness.values)
[['East South Central' 'Alabama' 2570.0 864.0 4887681]
 ['Pacific' 'Alaska' 1434.0 582.0 735139]
 ['Mountain' 'Arizona' 7259.0 2606.0 7158024]
 ['West South Central' 'Arkansas' 2280.0 432.0 3009733]
 ['Pacific' 'California' 109008.0 20964.0 39461588]
 ['Mountain' 'Colorado' 7607.0 3250.0 5691287]
 ['New England' 'Connecticut' 2280.0 1696.0 3571520]
 ['South Atlantic' 'Delaware' 708.0 374.0 965479]
 ['South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]
 ['South Atlantic' 'Florida' 21443.0 9587.0 21244317]
 ['South Atlantic' 'Georgia' 6943.0 2556.0 10511131]
 ['Pacific' 'Hawaii' 4131.0 2399.0 1420593]]




# Print the column index of homelessness
print(homelessness.columns)
Index(['region', 'state', 'individuals', 'family_members', 'state_pop'], dtype='object')




# Print the row index of homelessness
print(homelessness.index)
Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,
            49, 50],
           dtype='int64')




# Sort homelessness by region, then descending family members
homelessness_reg_fam = homelessness.sort_values(["region","family_members"] , ascending=[True, False])
print(homelessness_reg_fam)
                region                 state  individuals  family_members  state_pop
13  East North Central              Illinois       6752.0          3891.0   12723071
35  East North Central                  Ohio       6929.0          3320.0   11676341
22  East North Central              Michigan       5209.0          3142.0    9984072
32        Mid-Atlantic              New York      39827.0         52070.0   19530351
38        Mid-Atlantic          Pennsylvania       8163.0          5349.0   12800922
30        Mid-Atlantic            New Jersey       6048.0          3350.0    8886025
5             Mountain              Colorado       7607.0          3250.0    5691287
2             Mountain               Arizona       7259.0          2606.0    7158024
44            Mountain                  Utah       1904.0           972.0    3153550
21         New England         Massachusetts       6811.0         13257.0    6882635
6          New England           Connecticut       2280.0          1696.0    3571520
4              Pacific            California     109008.0         20964.0   39461588
47             Pacific            Washington      16424.0          5880.0    7523869
9       South Atlantic               Florida      21443.0          9587.0   21244317
8       South Atlantic  District of Columbia       3770.0          3134.0     701547
33      South Atlantic        North Carolina       6451.0          2817.0   10381615
10      South Atlantic               Georgia       6943.0          2556.0   10511131
23  West North Central             Minnesota       3993.0          3250.0    5606249
25  West North Central              Missouri       3776.0          2107.0    6121623
15  West North Central                  Iowa       1711.0          1038.0    3148618
16  West North Central                Kansas       1443.0           773.0    2911359






# Select only the individuals and state columns,
# df["xxx"] dataframedeki tum rowlari xxx columnu ozelinde getirir. 
# df[filter] -> filtreleme kuralina gore row'lari sectigimiz kod parcasi
# df[] bracket icine yazdimiz sey "" icinde ise column olarak algilar onun disinda yazdigimiz her sey filtreleme conditiondur. T/F
# df[[]] -> dataframedeki tum rowlari [] listesi icine yazdimiz column listesi ile listeler.
ind_state = homelessness[["individuals","state"]]
print(ind_state)
    individuals                 state
0        2570.0               Alabama
1        1434.0                Alaska
2        7259.0               Arizona
3        2280.0              Arkansas
4      109008.0            California
5        7607.0              Colorado
6        2280.0           Connecticut
7         708.0              Delaware
8        3770.0  District of Columbia
9       21443.0               Florida
10       6943.0               Georgia
11       4131.0                Hawaii
12       1297.0                 Idaho





# Filter for rows where individuals is greater than 10000
ind_gt_10k = homelessness[homelessness["individuals"] > 10000]
print(ind_gt_10k)
                region       state  individuals  family_members  state_pop
4              Pacific  California     109008.0         20964.0   39461588
9       South Atlantic     Florida      21443.0          9587.0   21244317
32        Mid-Atlantic    New York      39827.0         52070.0   19530351
37             Pacific      Oregon      11139.0          3337.0    4181886
43  West South Central       Texas      19199.0          6111.0   28628666
47             Pacific  Washington      16424.0          5880.0    7523869





# Filter for rows where family_members is less than 1000 and region is Pacific
is_pacific = homelessness["region"] == 'Pacific'
is_less1000 = homelessness["family_members"] < 1000
fam_lt_1k_pac = homelessness[is_pacific & is_less1000]
print(fam_lt_1k_pac)
    region   state  individuals  family_members  state_pop
1  Pacific  Alaska       1434.0           582.0     735139





# Subset/Filter for rows in South Atlantic or Mid-Atlantic regions
is_SA_or_MA = homelessness["region"].isin(["South Atlantic","Mid-Atlantic"])
south_mid_atlantic = homelessness[is_SA_or_MA]
print(south_mid_atlantic)
            region                 state  individuals  family_members  state_pop
7   South Atlantic              Delaware        708.0           374.0     965479
8   South Atlantic  District of Columbia       3770.0          3134.0     701547
9   South Atlantic               Florida      21443.0          9587.0   21244317
10  South Atlantic               Georgia       6943.0          2556.0   10511131
20  South Atlantic              Maryland       4914.0          2230.0    6035802
30    Mid-Atlantic            New Jersey       6048.0          3350.0    8886025
32    Mid-Atlantic              New York      39827.0         52070.0   19530351
33  South Atlantic        North Carolina       6451.0          2817.0   10381615
38    Mid-Atlantic          Pennsylvania       8163.0          5349.0   12800922
40  South Atlantic        South Carolina       3082.0           851.0    5084156
46  South Atlantic              Virginia       3928.0          2047.0    8501286
48  South Atlantic         West Virginia       1021.0           222.0    1804291





# The Mojave Desert states
canu = ["California", "Arizona", "Nevada", "Utah"]
mojave_homelessness = homelessness[homelessness["state"].isin(canu)]
print(mojave_homelessness)

      region       state  individuals  family_members  state_pop
2   Mountain     Arizona       7259.0          2606.0    7158024
4    Pacific  California     109008.0         20964.0   39461588
28  Mountain      Nevada       7058.0           486.0    3027341
44  Mountain        Utah       1904.0           972.0    3153550






# Add total col as sum of individuals and family_members - total adinda yeni bor column yaratiyoruz!!!
homelessness["total"] = homelessness["individuals"] + homelessness["family_members"]
# Add p_individuals col as proportion of individuals
homelessness["p_individuals"] = homelessness["individuals"] / homelessness["total"]
print(homelessness)
      region                 state  individuals  family_members  state_pop     total  p_individuals
0   East South Central               Alabama       2570.0           864.0    4887681    3434.0          0.007
1              Pacific                Alaska       1434.0           582.0     735139    2016.0          0.004
2             Mountain               Arizona       7259.0          2606.0    7158024    9865.0          0.020
3   West South Central              Arkansas       2280.0           432.0    3009733    2712.0          0.006
4              Pacific            California     109008.0         20964.0   39461588  129972.0          0.296
5             Mountain              Colorado       7607.0          3250.0    5691287   10857.0          0.021
6          New England           Connecticut       2280.0          1696.0    3571520    3976.0          0.006
7       South Atlantic              Delaware        708.0           374.0     965479    1082.0          0.002
8       South Atlantic  District of Columbia       3770.0          3134.0     701547    6904.0          0.010
9       South Atlantic               Florida      21443.0          9587.0   21244317   31030.0          0.058
10      South Atlantic               Georgia       6943.0          2556.0   10511131    9499.0          0.019
11             Pacific                Hawaii       4131.0          2399.0    1420593    6530.0          0.011
12            Mountain                 Idaho       1297.0           715.0    1750536    2012.0          0.004
13  East North Central              Illinois       6752.0          3891.0   12723071   10643.0          0.018
14  East North Central               Indiana       3776.0          1482.0    6695497    5258.0          0.010
15  West North Central                  Iowa       1711.0          1038.0    3148618    2749.0          0.005
16  West North Central                Kansas       1443.0           773.0    2911359    2216.0          0.004
17  East South Central              Kentucky       2735.0           953.0    4461153    3688.0          0.007
18  West South Central             Louisiana       2540.0           519.0    4659690    3059.0          0.007
19         New England                 Maine       1450.0          1066.0    1339057    2516.0          0.004








# Import NumPy and create custom IQR function
import numpy as np
def iqr(column):
    return column.quantile(0.75) - column.quantile(0.25)
# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment
# df'in secilen rowlarina agg icersine parametre olarak verilen fonksiyonlar uygulanarak ozetlemeler yapilabilir.
print(sales[["temperature_c", "fuel_price_usd_per_l", "unemployment"]].agg([iqr,np.median]))
        temperature_c  fuel_price_usd_per_l  unemployment
iqr            16.583                 0.073         0.565
median         16.967                 0.743         8.099






# Sort sales_1_1 by date - bazi cumulative fonksyionlari uygulamadan once sortlamak sart. cummax mesela!
sales_1_1 = sales_1_1.sort_values("date")
# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col - secilen rowlara ait column verilerini toplayarak gider.
sales_1_1["cum_weekly_sales"] = sales_1_1["weekly_sales"].cumsum()
print(sales_1_1)

# Get the cumulative max of weekly_sales, add as cum_max_sales col
sales_1_1["cum_max_sales"] = sales_1_1["weekly_sales"].cummax()

# See the columns you calculated
print(sales_1_1[["date", "weekly_sales", "cum_weekly_sales", "cum_max_sales"]])


date  weekly_sales  cum_weekly_sales  cum_max_sales
0  2010-02-05      24924.50          24924.50       24924.50
1  2010-03-05      21827.90          46752.40       24924.50
2  2010-04-02      57258.43         104010.83       57258.43
3  2010-05-07      17413.94         121424.77       57258.43
4  2010-06-04      17558.09         138982.86       57258.43
5  2010-07-02      16333.14         155316.00       57258.43
6  2010-08-06      17508.41         172824.41       57258.43
7  2010-09-03      16241.78         189066.19       57258.43
8  2010-10-01      20094.19         209160.38       57258.43
9  2010-11-05      34238.88         243399.26       57258.43
10 2010-12-03      22517.56         265916.82       57258.43
11 2011-01-07      15984.24         281901.06       57258.43



# Drop duplicate store/department combinations
store_depts = sales.drop_duplicates(subset=["store","department"])
print(store_depts.head())

store type  department       date  weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment
0       1    A           1 2010-02-05      24924.50       False          5.728                 0.679         8.106
12      1    A           2 2010-02-05      50605.27       False          5.728                 0.679         8.106
24      1    A           3 2010-02-05      13740.12       False          5.728                 0.679         8.106
36      1    A           4 2010-02-05      39954.04       False          5.728                 0.679         8.106
48      1    A           5 2010-02-05      32229.38       False          5.728                 0.679         8.106


# Subset the rows that are holiday weeks and drop duplicate dates
holiday_dates = sales[sales["is_holiday"] == True].drop_duplicates(subset="date")

    store type  department       date  weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment
498       1    A          45 2010-09-10         11.47        True         25.939                 0.678         7.787
691       1    A          77 2011-11-25       1431.00        True         15.633                 0.855         7.866
2315      4    A          47 2010-02-12        498.00        True         -1.756                 0.680         8.623
6735     19    A          39 2012-09-07         13.41        True         22.333                 1.077         8.193
6810     19    A          47 2010-12-31       -449.00        True         -1.861                 0.881         8.067
6815     19    A          47 2012-02-10         15.00        True          0.339                 1.011         7.943
6820     19    A          48 2011-09-09        197.00        True         20.156                 1.038         7.806

# Print date col of holiday_dates
print(holiday_dates["date"])

498    2010-09-10
691    2011-11-25
2315   2010-02-12
6735   2012-09-07
6810   2010-12-31
6815   2012-02-10
6820   2011-09-09
Name: date, dtype: datetime64[ns]

print(stores.head())
   store type
0      1    A
1      2    A
2      4    A
3      6    A
4     10    B


store_counts = stores["type"].value_counts()
print(store_counts)
A    11
B     1
Name: type, dtype: int64


store_props = stores["type"].value_counts(normalize=True)
print(store_props)
A    0.917
B    0.083
Name: type, dtype: float64


print(departments.head())
   store  department
0      1           1
1      1           2
2      1           3
3      1           4
4      1           5

dept_counts_sorted = departments["store"].value_counts(sort=True)
print(dept_counts_sorted)
19    79
13    79
27    78
20    78
4     78
2     78
14    77
10    77
6     77
1     77
31    76
39    75
Name: store, dtype: int64

print(sales.head())
   store type  department       date  weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment
0      1    A           1 2010-02-05      24924.50       False          5.728                 0.679         8.106
1      1    A           1 2010-03-05      21827.90       False          8.056                 0.693         8.106
2      1    A           1 2010-04-02      57258.43       False         16.817                 0.718         7.808
3      1    A           1 2010-05-07      17413.94       False         22.528                 0.749         7.808
4      1    A           1 2010-06-04      17558.09       False         27.050                 0.715         7.808


sales_all = sales["weekly_sales"].sum()
 print(sales_all)
256894718.89999998

In [4]: sales_A = sales[sales["type"] == "A"]["weekly_sales"].sum()
        print(sales_A)
233716315.01

In [5]: sales_B = sales[sales["type"] == "B"]["weekly_sales"].sum()
        print(sales_B)
23178403.89

In [6]: sales_C = sales[sales["type"] == "C"]["weekly_sales"].sum()
        print(sales_C)
0.0

In [9]: sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all
        print(sales_propn_by_type)
[0.9097747 0.0902253 0.       ]


In [2]: sales_by_type = sales.groupby("type")["weekly_sales"].sum()
        print(sales_by_type)
type
A    2.337e+08
B    2.318e+07
Name: weekly_sales, dtype: float64

In [32]: sales_propn_by_type = sales_by_type / sales_by_type.sum()
         print(sales_propn_by_type)
type
A    0.91
B    0.09
Name: weekly_sales, dtype: float64

sales_by_type_is_holiday = sales.groupby(["type","is_holiday"])["weekly_sales"].sum()
        print(sales_by_type_is_holiday)
type  is_holiday
A     False         2.337e+08
      True          2.360e+04
B     False         2.318e+07
      True          1.621e+03
Name: weekly_sales, dtype: float64

        # For each store type, aggregate weekly_sales: get min, max, mean, and median
        sales_stats = sales.groupby("type")["weekly_sales"].agg([min,max,np.median])
        
        # Print sales_stats
        print(sales_stats)
         min        max    median
type                             
A    -1098.0  293966.05  11943.92
B     -798.0  232558.51  13336.08


unemp_fuel_stats = sales.groupby("type")["unemployment","fuel_price_usd_per_l"].agg([min,max,np.median])
        
        # Print unemp_fuel_stats
        print(unemp_fuel_stats)
     unemployment               fuel_price_usd_per_l              
              min    max median                  min    max median
type                                                              
A           3.879  8.992  8.067                0.664  1.107  0.735
B           7.170  9.765  9.199                0.760  1.108  0.803


# Import NumPy as np
        import numpy as np
        
        # Pivot for mean and median weekly_sales for each store type
        mean_med_sales_by_type = sales.pivot_table(values= "weekly_sales" , index= "type" , aggfunc=[np.mean, np.median])
        
        
        # Print mean_med_sales_by_type
        print(mean_med_sales_by_type)
             mean       median
     weekly_sales weekly_sales
type                          
A       23674.667     11943.92
B       25696.678     13336.08

import numpy as np
        # Pivot for mean weekly_sales by store type and holiday 
        mean_sales_by_type_holiday = sales.pivot_table(values= "weekly_sales" , index= "type", columns= "is_holiday")
        
        # Print mean_sales_by_type_holiday
        print(mean_sales_by_type_holiday)

is_holiday      False    True 
type                          
A           23768.584  590.045
B           25751.981  810.705


print(sales.pivot_table(values="weekly_sales", index="department", columns="type", fill_value=0, margins=True))
type                A           B        All
department                                  
1           30961.725   44050.627  32052.467
2           67600.159  112958.527  71380.023
3           17160.003   30580.655  18278.391
4           44285.399   51219.654  44863.254
5           34821.011   63236.875  37189.000
...               ...         ...        ...
96          21367.043    9528.538  20337.608
97          28471.267    5828.873  26584.401
98          12875.423     217.428  11820.590
99            379.124       0.000    379.124
All         23674.667   25696.678  23843.950


# Make a list of cities to subset on
cities = ['Moscow','Saint Petersburg']

# Subset temperatures using square brackets
print(temperatures[temperatures["city"].isin(cities)])

 date              city country  avg_temp_c
10725 2000-01-01            Moscow  Russia      -7.313
10726 2000-02-01            Moscow  Russia      -3.551
10727 2000-03-01            Moscow  Russia      -1.661
10728 2000-04-01            Moscow  Russia      10.096
10729 2000-05-01            Moscow  Russia      10.357
...          ...               ...     ...         ...
13360 2013-05-01  Saint Petersburg  Russia      12.355
13361 2013-06-01  Saint Petersburg  Russia      17.185
13362 2013-07-01  Saint Petersburg  Russia      17.234
13363 2013-08-01  Saint Petersburg  Russia      17.153
13364 2013-09-01  Saint Petersburg  Russia         NaN

[330 rows x 4 columns]

# Subset temperatures_ind using .loc[]
print(temperatures_ind.loc[cities])
                       date country  avg_temp_c
city                                           
Moscow           2000-01-01  Russia      -7.313
Moscow           2000-02-01  Russia      -3.551
Moscow           2000-03-01  Russia      -1.661
Moscow           2000-04-01  Russia      10.096
Moscow           2000-05-01  Russia      10.357
...                     ...     ...         ...
Saint Petersburg 2013-05-01  Russia      12.355
Saint Petersburg 2013-06-01  Russia      17.185
Saint Petersburg 2013-07-01  Russia      17.234
Saint Petersburg 2013-08-01  Russia      17.153
Saint Petersburg 2013-09-01  Russia         NaN

[330 rows x 3 columns]


# Index temperatures by country & city
        indeksler = ["country","city"]
        temperatures_ind = temperatures.set_index(indeksler)
        
        # List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore
        rows_to_keep = [("Brazil", "Rio De Janeiro"),("Pakistan", "Lahore")]
        
        # Subset for rows to keep
        print(temperatures_ind.loc[rows_to_keep])
                              date  avg_temp_c
country  city                                 
Brazil   Rio De Janeiro 2000-01-01      25.974
         Rio De Janeiro 2000-02-01      26.699
         Rio De Janeiro 2000-03-01      26.270
         Rio De Janeiro 2000-04-01      25.750
         Rio De Janeiro 2000-05-01      24.356
...                            ...         ...
Pakistan Lahore         2013-05-01      33.457
         Lahore         2013-06-01      34.456
         Lahore         2013-07-01      33.279
         Lahore         2013-08-01      31.511
         Lahore         2013-09-01         NaN

[330 rows x 2 columns]







In [3]: print(temperatures_ind.sort_index(level=["country","city"], ascending=[True, False]))
                         date  avg_temp_c
country     city                         
Afghanistan Kabul  2000-01-01       3.326
            Kabul  2000-02-01       3.454
            Kabul  2000-03-01       9.612
            Kabul  2000-04-01      17.925
            Kabul  2000-05-01      24.658
...                       ...         ...
Zimbabwe    Harare 2013-05-01      18.298
            Harare 2013-06-01      17.020
            Harare 2013-07-01      16.299
            Harare 2013-08-01      19.232
            Harare 2013-09-01         NaN

[16500 rows x 2 columns]

##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################
##########################################################################################











P-values
Hypothesis testing -> To test a hypothesis based on a data and try to conclude a result using that data. 
The null hypothesis claims that the 'fenomen' we observed is something that is ordinary not extraordinary. Not a big fuss!!!

Eg; Null hypothesis can be Drugs A and B are completely the same alternative hypothesis is drugs are different.
Null hypothesis the coin that I have is an ordinary coin not a big deal. Alternative the coin I flip is extraordinary.
 
P value helps us decide whether to reject null hypothesis or fail to reject null hypothesis. We cant say accept null hypothesis.
Having a p-value small enough we can say that we can reject null hypothesis.

How to set a test
I flip a coin 2 times and both times i got heads; then i can say that i have a super special coin (this is a hypothesis)
In statistics we have to reverse it so my null hypothesis is my coin is no different than a random coin and try to reject with tests and
try to get small p-values during those tests.

If i try to test whether 2 drugs are different than each other. I assume that my drug is super special and cure any disase on world
Then my null hypothesis is these 2 drugs that i am going to test is no different than each other and try to reject it by gettin low p-val.


What is the meaning of having a test positive?
Eger bir coinin extraordinary olup olmadini test etmek istiyorsak null hipotez der ki this coin is ordinary. Eger coin gercekten ordinary cikarsa high p value test is pozitif.
eger yaptigim testte p value dusuk bulursam extraordinary cikar coin 
test fail eder? 

Eger 2 sample in 
It seems it is fail to reject the null hypothesis


Sensitivity (Recall - Prob. of detection) - Specificity
Sensitivity gercekten hasta olan insanlara odaklanir. Hasta olan bir insana hasta diyor ise testimiz bu True Pozitiftir. Eger hasta olan insana hasta degil seklinde
tani koyar ise testimiz bu da False Negatiftir. Sensitivity -> TP/TP+FN
Specificity ise hasta olmayan iansanlara odaklanir. Hasta olmayan bir insana hasta degil demek True Negatif'tir. Eger kisi hasta degil ise ve test hasta derse bu
da False Pozitif oluyor. Specificity -~ FP/FP+TN

Note that the terms "positive" and "negative" don't refer to the value of the condition of interest, but to its presence or absence; the condition itself could be a disease, 
so that "positive" might mean "diseased", while "negative" might mean "healthy".


P-values

Is used to decide whether a 'phenomenon' is extraordinary or not. We fail to see the difference from ordinary when we get Large p values.
If we compare 2 samples again and again; if those samples are coming from same population we expect to get large p-values and this will not be extraordinary for us
So our null hypothesis will be samples are coming from same population. Alternative hypothesis will be samples are coming from different populations
If we are getting large p-values (there is nothing extraordinary difference between 2 samples) we can say that we fail to reject null hypothesis (not accept)
Meaning we can not say that these 2 samples are coming from 2 different populations.
Even we are having samples from same population we are still going to have small p-values (<0.05) which is %5 of the times and those are false positives.
False positive means even we get the samples from same population we may get low p-values that tells us 
When we get the samples from different populations and we get large p-values (nothing special) those are classified as false negatives. 
This says us even we have large p-values (nothing extraordinary) those samples are actually coming from different samples and wrongly classified. Very costly. 


If the p value is large we can say that we fail to see difference between 2 things that we apply test (we cant say its same)
If the p value is small we can say that the things we are testing are 2 different things.
Take p value extremely small like 0.000001 when we would like to be super sure about the thing we test.

Sometimes we can get small p values (means results are pretty different) applying same test to different population. (false positive?)
This because of the population that we apply tests. Weird random things effect the result not the actual thing we test.

Getting a small p value when actually there is no difference on tests is called a false positive.(%5)(%5 is an assumption can be changed)

%5 of experiments where the only difference come from weird random things but not the thing we try to test generate a p values smaller
than 0.05 (false positive)

While a small p value helps us Drug A is different than Drug B it does not tells us exactly how different they are.

How to calculate p value
1) probablility of getting the result + 2) prob of getting similar results + 3) prob of getting rarer results.

For data that are discrete we can calculate probabilities so can calculate p-values easly. But for continous data? Histograms are used!!!

Bazen bir event'in olasiligi dusuk olsa bile bu eventi gozlemlemek o kadar da ozel bir sey olmayabilir. Mesela bir istatistiksel 
dagilimda mean e yakin bir olcum yaptigimizda bu olcume ait olaslik kendi basina gozlemlenme olasiligi dusuk bir olcum olsa da (olasilik) 
gozlemlenmesi cok da ozel bir durum olmayabilir. (p-value)

False discovery rate ; FDR -> weed out bad data that looks good! Benjamini Hochberg method to limit the number of false positives.
False negatives can be reduced by increasing the sample size. And false negative means when we try a test and if we get samples from 
2 different populations and we still get the p-values very large. Although 2 samples are different we cant reject the null 
hypothesis since we get large p-values.

When the samples come from the same distribution p-values are uniformly distributed. So we have false positives which consists %5 of
the measuraments that says even we got 2 samples from same distribution we still have low p-values that says samples are coming from
2 different populations. p-value says that this is an unlikely event that those samples are coming from same distribution/population.

When the samples are coming from 2 different populations we see that p-values are left skewed distributed. Most of the p-values are 
very small (which is less than 0.05) and this says us this very unlikely that these 2 mesurements are coming from the same population.
We still have large p-values which are false negatives although 2 samples are coming from different distributions we do not find it
oddly different from each other.

1vs 2 tailed tests
You have new cancer treatment. You hope that this treatment do better than the current treatment. You do a small test on 6 patients. Data shows you that in general 
people who takes new treatment gets better results if you compare it with people who get standart treatment. But in some individual cases people who takes new 
treatment may perform worse than some people who get current/ordinary treatment. So you apply statistics.
(Hypothesis new cancer treatment performs better(different?) than the ordinary cancer treatment. In stats jargon null hypothesis is new treatment is no different than 
current ordinary treatment!!! )

So you apply 1-tailed p-value test and you get the result 0.03 which is less than 0.05 so you can reject the null hypothesis and say that we reject the null hypothesis 
so we can say that new tratment performs different than the ordinary treatment. But what if you apply 2-tailed p-value test. It gives you 0.06 and this is greater than 
the threshold 0.05 so we fail to reject the null hypothesis. New treatment gives results that are not significantly different from current ordinary tests.






















